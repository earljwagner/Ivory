<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
<title>Ivory: Getting Started with TREC Disks 4-5</title>
<style type="text/css" media="screen">@import url( style.css );</style>
</head>

<body>

<div id="wrap">
<div id="container" class="one-column" >

<!-- header START -->

<div id="header">
<div id="caption">
<h1 id="title" style="color: white;">Ivory</h1>
<div id="tagline">A Hadoop toolkit for web-scale information retrieval research</div>
</div>

<div class="fixed"></div>

</div>

<!-- header END -->

<!-- navigation START -->

<div id="navigation">
<ul id="menus">

  <li class="page_item"><a class="home" title="Home" href="../index.html">Home</a></li>
  <li class="page_item"><a href="api/index.html" title="API">API</a></li>
  <li class="page_item"><a href="publications.html" title="Publications">Publications</a></li>
  <li class="page_item"><a href="regression.html" title="Experiments">Experiments</a></li>
  <li class="page_item"><a href="team.html" title="Team">Team</a></li>

</ul>

<div class="fixed"></div>

</div>

<!-- navigation END -->



<!-- content START -->

<div id="content">



	<!-- main START -->

	<div id="main">


<!--- START MAIN CONTENT HERE -->

<h2>Ivory: Getting Started with TREC Disks 4-5</h2>

<div class="post">
<div class="content">

<p>This tutorial provides a guide to batch and interactive retrieval
with Ivory on the venerable TREC
disks <a href="http://www.nist.gov/srd/nistsd22.htm">4</a>
and <a href="http://www.nist.gov/srd/nistsd23.htm">5</a> document
collection, which is distributed
by <a href="http://www.nist.gov/">NIST</a> and used in
many <a href="http://trec.nist.gov/">Text Retrieval Conferences</a>
(TRECs).  Although the collection is over a decade old, it is still
used as a starting point for information retrieval research.  This
guide will cover both indexing the collection, performing retrieval
runs with queries from the TREC 2004 robust track, and interactive
querying.</p>

</div></div>

<div class="post">
<h2>Getting the Collection</h2>
<div class="content">

<p>The first task is to obtain the collection (from NIST).  We're
assuming you have it in hand already.  A standard "view" of the disks
is to ignore the Congressional Record (CR) and Federal Register (FR),
so the collection is often written shorthand as TREC 45 (-CR,FR) or
something similar.</p>

<p>There are a total of 472,525 documents in the collection as
described above, distributed in a number of files;
see <a href="../data/trec/files.TREC45-noCRFR.txt">complete list of all
files</a>.  Since Hadoop doesn't work well will lots of small files,
the first step is to prepare the collection by concatenating all the
documents into a large file.  This is most easily done with a Perl or
Python script.  See this <a href="../data/trec/cat_all_docs.pl">simple
Perl script</a>, but it should be very easy to write your own.</p>

</div></div>

<div class="post">
<h2>Building the Inverted Index</h2>
<div class="content">

<p>The complete Ivory preprocssing and indexing pipeline is described
in detail <a href="pipeline.html">on this page</a>.  Here, we mostly
focus on <i>what to do</i>, as opposed to <i>how it works</i>.</p>

<p>Let's get started!  Prior to indexing, we must first preprocess the
collection.  The goal of this step is to create a compact
representation of the original document collection.  To preprocess the
collection, use the program
<code><b>ivory.driver.PreprocessTREC</b></code>. The program takes four 
command-line arguments:</p>
 
<ul>
 <li>[input-path] path to the document collection</li>
 <li>[index-path] path to index directory</li>
 <li>[num-mappers] number of mappers to run</li>
 <li>[num-reducers] number of reducers to run</li>
</ul>

<p>Here's a sample invocation (notice that the input path is
simple one large file):</p>

<pre>
hadoop jar ivory.jar ivory.driver.PreprocessTREC \
  /umd-lin/shared/collections/trec/trec4-5_noCRFR.xml /umd-lin/shared/indexes/trec 100 10
</pre>

<p>Once the collection has been preprocessed, we can use the program
<code><b>ivory.driver.BuildIPIndex</b></code> to build the index.  The program
takes three command-line arguments&mdash;the same four used in the
preprocessing step except the first.  Here's a sample invocation:</p>

<pre>
hadoop jar ivory.jar ivory.driver.BuildIPIndex /umd-lin/shared/indexes/trec 100 10
</pre>

<p>Next, let's build the document forward index. This forward index
provides a mechanism for accessing the actual document text.  It's
essentially a big lookup table of docnos to byte offsets in the
collection file on disk (and also the length of each document).</p>

<p>The program for doing this is actually included in
the <a href="http://cloud9lib.org">Cloud<sup><small>9</small></sup></a>
library.  Here's a sample invocation:</p>

<pre>
hadoop jar cloud9.jar edu.umd.cloud9.collection.trec.BuildTrecForwardIndex \
  /umd-lin/shared/collections/trec/trec4-5_noCRFR.xml /tmp/findex/ \
  /umd-lin/shared/collections/trec4-5_noCRFR.findex.dat \
  /umd-lin/shared/indexes/trec/docno-mapping.dat
</pre>

</div></div>

<div class="post">
<h2>Performing Batch Retrieval</h2>
<div class="content">

<p>Now we're ready to perform batch retrieval!  To demonstrate, we're
going to use topics from the TREC 2004 robust track.  In information
retrieval parlance, topics define information needs, from which the
actual queries derive.  In the <code>data/trec/</code> directory, you'll find
two configuration files:</p>

<ul>

  <li><a href="../data/trec/run.robust04.basic.xml"><code>data/trec/run.robust04.basic.xml</code></a>:
  retrieval models and parameters</li>

  <li><a href="../data/trec/queries.robust04.xml"><code>data/trec/queries.robust04.xml</code></a>:
  queries (TREC 2004 robust track)</li>

</ul>

<p>The first configuration file specifies six different models:</p>

<ul>

  <li><b>robust04-dir-base</b>: language modeling framework, Dirichlet
  prior, simple query likelihood.</li>

  <li><b>robust04-dir-sd</b>: language modeling framework, Dirichlet
  prior, sequential dependence model using MRFs.</li>

  <li><b>robust04-dir-fd</b>: language modeling framework, Dirichlet
  prior, full dependence model using MRFs.</li>

  <li><b>robust04-bm25-base</b>: <i>bm25</i> term weighting, simple
  bag-of-words queries.</li>

  <li><b>robust04-bm25-sd</b>: sequential dependence model using MRFs,
  with <i>bm25</i> term weighting.</li>

  <li><b>robust04-bm25-fd</b>: full dependence model using MRFs,
  with <i>bm25</i> term weighting.</li>

</ul>

<p>You might want to tweak the index location and output paths
in <a href="../data/trec/run.robust04.basic.xml"><code>data/trec/run.robust04.basic.xml</code></a>,
specifying where the index resides and where the retrieval results go.
After that, put both configuration files in HDFS.  The
class <b><code>ivory.smrf.retrieval.RunQueryHDFS</code></b> performs a
batch <i>ad hoc</i> retrieval run: it takes the two XML configuration
files as input and produces results in standard TREC format on HDFS.
Here's a sample invocation:</p>

<pre>
hadoop jar ivory.jar ivory.smrf.retrieval.RunQueryHDFS \
  /umd-lin/jimmylin/run.robust04.basic.xml /umd-lin/jimmylin/queries.robust04.xml
</pre>

<p>Here's what happens: <code>RunQueryHDFS</code> executes a Hadoop
job with a single mapper, and inside the mapper a retrieval engine is
instantiated.  The retrieval engine loads in the configuration files
and performs batch <i>ad hoc</i> retrieval, reading indexes directly
from HDFS (more on this below).</p>

<p>After the job is complete, grab the output files (one for each
model) from HDFS&mdash;these are results in standard TREC format.  The
first column is the topic number, the third column is the TREC
document identifier, the fourth column is the rank order, and the
fifth column is the score (log probability, in this case).  Document
relevance information (qrels) is included below:</p>

<ul>

  <li><a href="../data/trec/qrels.robust04.noCR.txt">data/trec/qrels.robust04.noCR.txt</a>:
  original qrels file; does not contain CR documents, but contains FR
  documents.</li>

  <li><a href="../data/trec/qrels.robust04.noCRFR.txt">data/trec/qrels.robust04.noCRFR.txt</a>:
  modified qrels file; does not contain CR and FR documents.</li>

</ul>

<p>With
the <a href="http://trec.nist.gov/trec_eval/index.html">trec_eval</a>
program, you should be able to evaluate the runs.  See
our <a href="regression.html#trec">experimental results page (under "Basic models")</a> for the
effectiveness numbers you should be getting.</p>

<p>Alternatively, you may want to run things locally (i.e., from a
local machine), instead of remotely on the cluster.  In that case, you
should copy the index over to your local machine.  The
class <b><code>ivory.smrf.retrieval.RunQueryLocal</code></b> performs
batch <i>ad hoc</i> retrieval on a local index.  Pass the XML
configuration files as command-line parameters, just like
RunQueryHDFS.</p>

<p>Ivory contains a generic launch script that automatically includes
all the right classpaths, automatically generated by Ant.  Open up a
shell and change directory into Ivory.  Type:</p>

<pre>
ant
</pre>

<p>Ant should automatically build ivory.jar and create a launch script
in <code>etc/</code>, either <code>run.sh</code>
or <code>run.bat</code> depending on your operating system.</p>

<p>On the command line, here's the invocation to perform an <i>ad
hoc</i> retrieval run:</p>

<pre>
etc/run.sh ivory.smrf.retrieval.RunQueryLocal \
  docs/data/trec/run.robust04.basic.xml docs/data/trec/queries.robust04.xml
</pre>

<p>You should get exactly the same results as with RunQueryHDFS.</p>

</div></div>


<div class="post">
<h2>Searching Interactively</h2>
<div class="content">

<p>Now, what about interactive retrieval?  We of course know that
MapReduce is designed for large batch jobs and is not suitable for
real-time interactive applications.  Specifically, HDFS is designed
for high-throughput streaming reads, not low-latency access to
(relatively) small amounts of information.  But of course, interactive
retrieval requires rapid access to postings corresponding to query
terms...</p>

<p>The standard solution is to pull the indexes out of HDFS into
another architecture that supports low-latency operations
(e.g., <a href="http://katta.sourceforge.net/about">Katta</a> is a
framework for managing distributed Lucene indexes).  However, this
presents a data-management challenging (copying large index files
around), and naturally, you lose the benefits of HDFS (e.g.,
redundancy through replication).</p>

<p>However, we've been playing with retrieval engines that directly
read postings from HDFS.  This may at first seem like a stupid idea,
but this approach does have merits.  Sure, most likely you'll be
reading postings from a remote datanode, but you gain the benefit of a
homogeneous environment (everything is Hadoop).  Early indications
suggest that this architecture give reasonable performance (we're
working on some ideas that we believe will make this architecture just
as fast as the alternative).</p>

<p>In summary, the Ivory retrieval architecture looks like the
following:</p>

<p><img width="450" src="images/retrieval-hdfs.png" alt="Retrieval Server reading directly from HDFS" /></p>

<p>What we've done is folded a retrieval engine into a webapp.  This
is easy because Hadoop already
uses <a href="http://www.mortbay.org/jetty/">Jetty</a> for its
webapps.  Furthermore, we folded the webapp inside a mapper (albeit a
degenerate one), so that we launch the retrieval sever like any other
Hadoop job.</p>

<p>Here are two configuration files for starting the server (copy over
to HDFS):</p>

<ul>

  <li><a href="../data/trec/server.trec.ql.xml">data/trec/server.trec.ql.xml</a>:
    configuration file for launching a query-likelihood server.</li>

  <li><a href="../data/trec/server.trec.bm25.xml">data/trec/server.trec.bm25.xml</a>:
  configuration file for launching a bm25 server.</li>

</ul>

<p>Each configuration file specifies the index location and also the
location of the forward index into the document collection (so you can
actually examine the retrieved results).  The
class <b><code>ivory.server.RunDistributedRetrievalServers</code></b>
launches the server.  Here's a sample invocation:</p>

<pre>
$ hadoop jar ivory.jar ivory.server.RunDistributedRetrievalServers /umd-lin/jimmylin/server.trec.ql.xml /tmp/config
10/05/23 23:00:28 INFO server.RunDistributedRetrievalServers: Reading configuration to determine number of servers to launch:
10/05/23 23:00:28 INFO server.RunDistributedRetrievalServers:  - sid: trec
10/05/23 23:00:28 INFO server.RunDistributedRetrievalServers: Writing configuration to: /tmp/config/config-1.txt
10/05/23 23:01:36 INFO mapred.FileInputFormat: Total input paths to process : 1
10/05/23 23:01:39 INFO server.RunDistributedRetrievalServers: Waiting for servers to start up...
10/05/23 23:01:49 INFO server.RunDistributedRetrievalServers:  ...
10/05/23 23:01:59 INFO server.RunDistributedRetrievalServers:  ...
10/05/23 23:02:10 INFO server.RunDistributedRetrievalServers:  ...
10/05/23 23:02:10 INFO server.RunDistributedRetrievalServers: All servers ready!
10/05/23 23:02:10 INFO server.RunDistributedRetrievalServers: Host information:
10/05/23 23:02:10 INFO server.RunDistributedRetrievalServers:  sid=trec, XX.XXX.X.XX:7001
</pre>

<p>The first command-line argument is the path to the configuration
file in HDFS.  The second argument is the path of a temporary
directory in HDFS.  (As a side note, you can start up multiple
retrieval servers simultaneously but simply specifying multiple index
locations.)</p>

<p>What happens is that the mapper starts up, initializes the
retrieval engine, and fires up a webapp by creating a Jetty HTTP
server listening on a particular port.  Of course, we have no idea
which cluster node ran the mapper, so the webapp writes out its
hostname and port to HDFS.  Meanwhile, the client submitting the job
polls HDFS waiting for the file to appear.  Once it does, the client
reads the config file and tells you where the webapp is.  If you
navigate to that hostname/port in a browser, you'll get access to a
since search interface where you can put in a query and look at
retrieved documents.</p>

<p>Finally, there's a
class <b><code>ivory.server.RunLocalRetrievalServer</code></b> that
supports interactive retrieval on a local machine.  Here's a sample
invocation:</p>

<pre>
$ etc/run.sh ivory.server.RunLocalRetrievalServer docs/data/trec/server.trec.ql.xml 9000
10/05/24 00:54:15 INFO server.RunLocalRetrievalServer: Reading configuration...
10/05/24 00:54:15 INFO server.RetrievalServer: Initializing RetrievalServer for "trec"...
10/05/24 00:54:15 INFO server.RetrievalServer:  - sid: trec
10/05/24 00:54:15 INFO server.RetrievalServer:  - index: /umd-lin/shared/indexes/trec
10/05/24 00:54:15 INFO server.RetrievalServer:  - findex: /umd-lin/shared/collections/trec4-5_noCRFR.findex.dat
10/05/24 00:54:15 INFO util.RetrievalEnvironment: Loading doclengths table...
10/05/24 00:54:15 INFO data.DocLengthTable: Docno offset: 0
10/05/24 00:54:15 INFO data.DocLengthTable: Number of docs: 472525
10/05/24 00:54:16 INFO data.DocLengthTable: Total of 472525 doclengths read
10/05/24 00:54:16 INFO util.RetrievalEnvironment: IndexPath: /umd-lin/shared/indexes/trec
10/05/24 00:54:16 INFO util.RetrievalEnvironment: PostingsType: ivory.data.PostingsListDocSortedPositional
10/05/24 00:54:16 INFO util.RetrievalEnvironment: Collection document count: 472525
10/05/24 00:54:16 INFO util.RetrievalEnvironment: Collection length: 134888069
10/05/24 00:54:16 INFO util.RetrievalEnvironment: Tokenizer: ivory.util.GalagoTokenizer
10/05/24 00:54:16 INFO util.RetrievalEnvironment: Loading postings index...
10/05/24 00:54:16 INFO util.RetrievalEnvironment: Number of terms: 978058
10/05/24 00:54:16 INFO util.RetrievalEnvironment: Done!
10/05/24 00:54:21 INFO server.RetrievalServer: RetrievalServer successfully initialized.
10/05/24 00:54:21 INFO server.RetrievalServer: Staring server...
10/05/24 00:54:21 INFO mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log)
10/05/24 00:54:21 INFO mortbay.log: jetty-6.1.14
10/05/24 00:54:22 INFO mortbay.log: Started SocketConnector@0.0.0.0:9000
10/05/24 00:54:22 INFO server.RetrievalServer: Server successfully started!
</pre>

<p>Once the program starts, you can navigate
to <code>http://localhost:9000/</code> in a browser and access a simple
search interface.  It's important to note that you're running this
program outside of MapReduce, so that's why you want to use the launch
script <code>etc/run.sh</code>, which is automatically generated by
Ant.</p>

<p>And that's it.  We've covered indexing, batch retrieval, and
interactive retrieval.  Enjoy!</p>

<p style="padding-top: 25px"><a href="../index.html">Back to main page</a></p>

</div></div>

<!--- END MAIN CONTENT HERE -->

	</div>

	<!-- main END -->



		<div class="fixed"></div>

</div>

<!-- content END -->

<!-- footer START -->

<div id="footer">
<div id="copyright">
Last updated:
<script type="text/javascript">
<!--//
document.write(document.lastModified);
//-->
</script>
</div>

<div id="themeinfo">
Adapted from a WordPress Theme by <a href="http://www.neoease.com/">NeoEase</a>. Valid <a href="http://validator.w3.org/check?uri=referer">XHTML 1.1</a> and <a href="http://jigsaw.w3.org/css-validator/check/referer?profile=css3">CSS 3</a>.	</div>

</div>

<!-- footer END -->



</div>

<!-- container END -->

</div>

<!-- wrap END -->

</body>
</html>
